<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A repository for my thoughts"><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=icon type=image/x-icon href=/favicon.ico><link rel=stylesheet href=/css/style.min.css><meta name=title property=”og:title” content="A Peek Into the Beam | Haile (ሐይሌ)"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Peek Into the Beam | Haile (ሐይሌ)"><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta name=twitter:creator content="@hailelagi"><title>A Peek Into the Beam</title></head><body><header id=banner><h2><a href=https://www.hailelagi.com/>Haile (ሐይሌ)</a></h2><nav><ul><li><a href=/bookshelf title=bookshelf>bookshelf</a></li><li><a href=https://www.github.com/hailelagi title=github>github</a></li><li><a href=/notes title=writing>writing</a></li></ul></nav></header><main id=content><article><header id=post-header><h1>A Peek Into the Beam</h1><time>March 29, 2022</time><meta name=twitter:card content="summary">
<meta property="og:url" content="https://www.hailelagi.com/"><meta property="og:image" content="/favicon-32x32.png"><meta itemprop=image content="/favicon-32x32.png"><meta name=twitter:image content="/favicon-32x32.png"><meta name=twitter:image:src content="/favicon-32x32.png"></header><p>A long time ago, you would give a computer an intensive set of instructions - in assembly or something more sane, and
it would compute these instructions one by one, but while it did that - it would “freeze up” you could not really do much
else with it. At the time, computer hardware was pretty limited, it had a single CPU core
(something that executes instruction sets) which did pretty much everything, one by one - computer scientists were not particularly
satisfied with this, and they <a href=https://en.wikipedia.org/wiki/Mutual_exclusion>found a solution</a>.</p><p>In essence, execution of two or more computations at the same time is possible - given it is guaranteed that both read
data from the same source, but not writing which could lead to inconsistency, commonly known as a <em>data race</em> or <em>race condition</em>.
Today our computers have multiple cores - they can do a lot more stuff than they <a href=https://en.wikipedia.org/wiki/Moore%27s_law>used to</a>,
but we need some way to guarantee or make it really hard for this to happen.</p><p>The world of concurrency is fascinating, lots of languages design mechanisms around this problem known as
<strong>concurrency primitives</strong>, allowing software creators to fashion applications and software systems that perform much better
than their sequential alternative, however we are most interested in a cursory glance into the BEAM
(Erlang’s virtual machine). For brief context, a virtual machine is just software - an abstraction over the basic
hardware of a computer allowing a layer of execution on top of it. A common example being the Java Virtual Machine (JVM).
The elixir/erlang source code is parsed and transformed into a set of intermediary files prefixed with <code>.beam</code> that the
virtual machine can understand known as bytecode, via the <code>C</code> programming language. From here it is translated into
assembly/machine instruction bits. <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p><strong>source code</strong> &mdash;> <strong>c interface</strong> &mdash;> <strong>bytecode</strong></p><p>Most of the interesting <a href=https://en.wikipedia.org/wiki/Actor_model>concurrency primitives</a> that erlang/elixir provide
are built on top of the <a href=https://ferd.ca/it-s-about-the-guarantees.html>guarantees</a> this virtual machine provides such
as immutable state. The single basic unit being a process <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> -
an isolated sequential unit of computation which is managed by a scheduler an important construct.</p><p>NOTE: you may think of a process <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> as a kind of &ldquo;green thread&rdquo;,
if familiar with the concept. Otherwise thinking of them
as an abstract unit of sequential computation is fine.</p><h2 id=erlangs-scheduler>Erlang’s scheduler</h2><p>The scheduler within the BEAM runtime (not an <a href=https://en.wikipedia.org/wiki/Scheduling_(computing)>operating system scheduler</a>),
talks to the operating system via <a href=https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/4_Threads.html>threads</a> and
manages the <a href=https://hamidreza-s.github.io/erlang/scheduling/real-time/preemptive/migration/2016/02/09/erlang-scheduler-details.html>how and when</a>
of computations (processes - in the vm). It does something called <em>preemptive scheduling</em> which requires making
a nuanced trade off - all processes are treated as equal(unless a priority is set) and given a tiny block of time/memory
to execute, whether this is enough for a process is irrelevant. It sacrifices the efficient allocation of resources to
processes that need it most to make some important guarantees which make fault tolerance possible:</p><ol><li>High availability</li><li>Isolated failure states</li></ol><p>This constant <em>context switching</em> gives guarantees creating a system that is dependable - allowing the creation
of processes that inspect others, we can leverage this information to make intelligent deductions about what is happening
within the system at runtime and design strategies to deal with and understand crashes and fail states,
while also providing concurrent primitives that naturally scale across distributed systems,
changing very little about the core system.</p><p>A typical illustration of erlang&rsquo;s introspective superpower is <code>:observer</code> which ships by default. Pop <code>:observer.start()</code>
into any <code>iex</code> session and watch the magic.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>user@my-pc $ iex
</span></span><span class=line><span class=cl>iex<span class=o>(</span>1<span class=o>)</span>&gt; :observer.start<span class=o>()</span>
</span></span></code></pre></div><p><img src=/observer.png alt="Observer showing scheduling"></p><p>You can see the schedulers at work by spinning up a few short-lived processes which begin their lifetime<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>
with about <a href=https://en.wikipedia.org/wiki/Word_(computer_architecture)>326 words of memory</a> (approximately 0.65 kilobytes)
which can <a href=https://www.erlang.org/doc/man/erts_alloc.html>grow</a> on a stack or heap.</p><p>Here you have <code>self()</code> as the <code>iex</code> session process, creating another process that it communicates with:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-elixir data-lang=elixir><span class=line><span class=cl><span class=n>iex</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>&gt;</span> <span class=n>current</span> <span class=o>=</span> <span class=n>self</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>iex</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>&gt;</span> <span class=n>child</span> <span class=o>=</span> <span class=n>spawn</span><span class=p>(</span><span class=k>fn</span> <span class=o>-&gt;</span> 
</span></span><span class=line><span class=cl>  <span class=n>send</span><span class=p>(</span><span class=n>current</span><span class=p>,</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1># new identifier process created by spawn</span>
</span></span><span class=line><span class=cl>    <span class=n>self</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=c1># any arbitrary sequential computation</span>
</span></span><span class=line><span class=cl>    <span class=c1># looping, control flow, anything :O</span>
</span></span><span class=line><span class=cl>    <span class=mi>1</span> <span class=o>+</span> <span class=mi>1</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=k>end</span><span class=p>)</span>
</span></span></code></pre></div><p>We can then leverage the high level <code>Process</code> library for convenience, to create more processes, thousands or even millions<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>
if need be:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-elixir data-lang=elixir><span class=line><span class=cl><span class=n>child_two</span> <span class=o>=</span> <span class=nc>Process</span><span class=o>.</span><span class=n>spawn</span><span class=p>(</span><span class=k>fn</span> <span class=o>-&gt;</span> <span class=mi>1</span> <span class=o>+</span> <span class=mi>2</span> <span class=k>end</span><span class=p>,</span> <span class=p>[</span><span class=ss>:monitor</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>child_three</span> <span class=o>=</span> <span class=nc>Process</span><span class=o>.</span><span class=n>spawn</span><span class=p>(</span><span class=k>fn</span> <span class=o>-&gt;</span> <span class=mi>1</span> <span class=o>+</span> <span class=mi>3</span> <span class=k>end</span><span class=p>,</span> <span class=p>[</span><span class=ss>:monitor</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>child_four</span> <span class=o>=</span> <span class=nc>Process</span><span class=o>.</span><span class=n>spawn</span><span class=p>(</span><span class=k>fn</span> <span class=o>-&gt;</span> <span class=mi>1</span> <span class=o>+</span> <span class=mi>4</span> <span class=k>end</span><span class=p>,</span> <span class=p>[</span><span class=ss>:monitor</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>child_five</span> <span class=o>=</span> <span class=nc>Process</span><span class=o>.</span><span class=n>spawn</span><span class=p>(</span><span class=k>fn</span> <span class=o>-&gt;</span> <span class=mi>1</span> <span class=o>+</span> <span class=mi>5</span> <span class=k>end</span><span class=p>,</span> <span class=p>[</span><span class=ss>:monitor</span><span class=p>])</span>
</span></span></code></pre></div><p>Processes have an <code>identity</code> via their <code>pid</code>, this is how they are aware of one another. The return value of each child
looks a little like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-elixir data-lang=elixir><span class=line><span class=cl><span class=c1># {#PID&lt;0.93.0&gt;, #Reference&lt;0.18808174.1939079169.202418&gt;}</span>
</span></span></code></pre></div><pre tabindex=0><code> NOTE: The actual pid and reference will be different on your machine).
</code></pre><p>When the scheduler(on one core) sees these concurrent tasks, it allocates some time and memory at runtime to <code>child</code>
and lets it run for a bit, if the process does not finish(an infinite loop for example), the scheduler moves on to
<code>child_two</code> and so on, checking up on each process, computing a bit. Processes can be namespaced in a
<a href=https://hexdocs.pm/elixir/1.13/Registry.html>local registry</a> for a single node to avoid passing around pids.
Scheduling across multiple nodes works the same way, only you&rsquo;d need a different way to manage the global name space of
running processes such as erlang&rsquo;s <code>:global</code>, <a href=https://github.com/uwiger/gproc>gproc</a> or <a href=https://github.com/bitwalker/swarm>swarm</a>,
depending on whether the system requires high availability or consistency at the network layer.</p><pre tabindex=0><code>Note: There are likely many scheduler threads coordinating 
on this activity on a single core, and at least one per
operating system process. Further details are subject to
which version of Erlang/OTP you&#39;re running and the implementation
has varied significantly over the years and may change.
</code></pre><p>High availability and isolated failure states are achieved via messages propagated through a web of processes. Leading to
interesting high level abstractions such as <a href=https://www.hailelagi.com/posts/dev/break-your-next-server/>supervisors</a>
and <a href=https://www.hailelagi.com/posts/dev/break-your-next-server/>agents</a> for handling local inter process state.</p><h2 id=its-all-about-tradeoffs>It&rsquo;s all about tradeoffs</h2><p>Elixir provides a beautiful modern language that allows you to leverage the amazing ecosystem and novel concurrency ideas
built into erlang, offering you the tools to create and design highly fault-tolerant, self-healing systems, sometimes
at the cost of <em>absolute runtime performance</em>. You can see this with need to replicate data structures and performing
computationally intensive tasks that would make sense to be processed sequentially. Do not despair however, you can
carefully poke a hole into the runtime through the C interface via
<a href="https://www.erlang.org/doc/tutorial/nif.html#:~:text=A%20NIF%20is%20a%20function,UNIX%2C%20DLL%20in%20Windows">Native Implementation Functions</a>,
whether in C++ or perhaps rust via <a href=https://github.com/rusterlium/rustler>rustler</a>. Or outsource this kind of
heavy-lifting if required to a service in a different language. Let&rsquo;s explore at a high level the conceptual
underpinnings of relatively more popular languages and how they stack up against the BEAM&rsquo;s approach.</p><h3 id=actor-model-vs-single-threadmultithreading-ruby-javascript-and-python>Actor Model vs Single Thread(multithreading) (Ruby, Javascript and Python)</h3><p>Ruby, Javascript and Python all have different concurrent models and implementations, however they share some important
similarities at a high enough level they can be grouped together. Ruby(MRI), CPython and Javascript&rsquo;s v8 runtime(node.js)
are all single threaded. Concurrency is achieved via a single Process(operating system) which has one large &ldquo;main&rdquo;
thread(where the runtime is loaded) which creates smaller threads of execution within a single context(system resources - memory etc).</p><pre tabindex=0><code>Note: You can infact create analagous threads of execution
beyond what is given but doing so is expensive and tricky.
</code></pre><p>Node.js in particular was especially optimised with this design early on. The limitations here are somewhat obvious, utilising
a multicore architecture is incredibly difficult and burdens the application developer with the nuances of lower level
details you&rsquo;ll simply not interface with in erlang/elixir. Ruby and Python historically however needed a mechanism called a Global Interpreter Lock(GIL)
to enforce/sync the runtime and make a data race impossible. This is often called a <em>mutual exclusion lock</em> and the algorithm
is plenty fascinating and deserving of its own article.</p><p>The primitives given are fairly similar - ruby gives you a <a href=https://ruby-doc.org/core-3.0.0/Thread.html>Thread class</a>
and <a href=https://ruby-doc.org/core-3.0.0/Fiber.html>Fibre</a> to create worker threads, node gives you access to the main
libuv<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup> managed <a href=https://nodejs.org/api/process.html#process>Process</a> and one for when you&rsquo;re
creating <a href=https://nodejs.org/api/worker_threads.html>worker threads</a>.</p><p>To utilise any form of thread parallel execution python provides a <a href=https://docs.python.org/3/library/multiprocessing.html>library interface</a>,
ruby core has been experimenting with and recently released an actor model inspired mechanism called <a href=https://docs.ruby-lang.org/en/3.0/Ractor.html>Ractor</a>.</p><p>In practice, when creating say a web server with these languages an <code>Event Loop</code><sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup> <sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> <sup id=fnref1:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup> <sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> handles the heavy lifting within the main thread, resources are simply not shared
and asynchronous failures caught with lots and lots of defensive programming.</p><h3 id=actor-model-vs-communicating-sequential-processes-goroutines>Actor Model vs Communicating sequential processes (goroutines)</h3><p>In some ways erlang and go share some features of their concurrent model - both leveraging the symmetric multiprocessing
<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup> architecture with the key difference eloquently expressed by a deceptively simple philosophy:</p><pre tabindex=0><code>Do not communicate by sharing memory; 
instead, share memory by communicating
</code></pre><p>Goroutines are analogous to &ldquo;processes&rdquo; being a lightweight &ldquo;unit&rdquo; of computation, however they have no identity(pid).
This isolation ensures the only way data moves is through a &ldquo;channel&rdquo;, a departure from the concept of a mailbox that
keeps track of immutable internal state, a channel serves the purpose of message passing between anonymous routines.</p><p>By opening a channel to some forgotten computation you can peek it&rsquo;s state and enforce synchronisation.</p><p>Resources are shared with carefully crafted rules. The analog of a supervisor being a &ldquo;monitor goroutine&rdquo;.
The sole writer of data in any cluster of spawned processes. This too is a form of message passing, just implemented
with a kind of artificial immutability for workers. Runtime failures (panics) are rarer in go, and instead errors treated
as values passed between goroutines. If panicked routines crash they inform the main go thread and the whole thing carries
along swimmingly.</p><p>Reasoning about concurrency systems is somewhat trickier here but allows for performance fine-tuning if you can enforce mutual
exclusion between goroutines. This freedom does come seemingly at a cost<sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup> which it seems all
languages that do not enforce immutable data structures and performance fine-tuning an exception rather than the norm,
but of course it all depends on context and use case.</p><p><em>Thanks to <a href=https://github.com/ponty96>Ayomide</a> and <a href=https://github.com/derhnyel>Daniel</a> for reviewing early drafts of this article.</em></p><h2 id=references>References</h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>fxn(medium): <a href=https://medium.com/@fxn/how-does-elixir-compile-execute-code-c1b36c9ec8cf>https://medium.com/@fxn/how-does-elixir-compile-execute-code-c1b36c9ec8cf</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>green threads(wikipedia): <a href=https://en.wikipedia.org/wiki/Green_threads>https://en.wikipedia.org/wiki/Green_threads</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Joe Armstrong(twitter): <a href=https://twitter.com/joeerl/status/1010485913393254401>https://twitter.com/joeerl/status/1010485913393254401</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Erlang documentation: <a href=https://www.erlang.org/doc/efficiency_guide/processes.html>https://www.erlang.org/doc/efficiency_guide/processes.html</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Erlang documentation: <a href=https://www.erlang.org/doc/reference_manual/processes.html>https://www.erlang.org/doc/reference_manual/processes.html</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>stackoverflow: <a href=https://stackoverflow.com/questions/2708033/technically-why-are-processes-in-erlang-more-efficient-than-os-threads>https://stackoverflow.com/questions/2708033/technically-why-are-processes-in-erlang-more-efficient-than-os-threads</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>node io: <a href=https://github.com/libuv/libuv>https://github.com/libuv/libuv</a>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>node event loop: <a href=https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/>https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/</a>&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>asyncio: <a href=https://docs.python.org/3/library/asyncio.html>https://docs.python.org/3/library/asyncio.html</a>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>RoR documentation: <a href=https://guides.rubyonrails.org/threading_and_code_execution.html>https://guides.rubyonrails.org/threading_and_code_execution.html</a>&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>symmetric multiprocessing: <a href=https://en.wikipedia.org/wiki/Symmetric_multiprocessing>https://en.wikipedia.org/wiki/Symmetric_multiprocessing</a>&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>go reference: <a href=https://go.dev/ref/mem>https://go.dev/ref/mem</a>&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></article></main><footer id=footer><a href=https://github.com/hailelagi/blog>Copyright © 2024 Haile Lagi</a><div><span>private inquiries: hailelagi[at]gmail.com</span></div><div><span>or informally(twitter/x): https://x.com/haile_lagi</span></div></footer></body></html>