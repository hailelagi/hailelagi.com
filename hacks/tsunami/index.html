<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A repository for my thoughts"><link rel="shortcut icon" href=https://www.hailelagi.com/favicon.ico><link rel=stylesheet href=/css/style.min.css><title>Making a Tsunami</title></head><body><header id=banner><h2><a href=https://www.hailelagi.com/>Haile (ሐይሌ)</a></h2><nav><ul><li><a href=/bookshelf title=bookshelf>bookshelf</a></li><li><a href=https://www.github.com/hailelagi title=github>github</a></li></ul></nav></header><main id=content><article><header id=post-header><h1>Making a Tsunami</h1><time>December 23, 2023</time></header><h1 id=introduction>Introduction</h1><p>This was one of the first really hard ambitious things I tried to build, but sadly because of
either a lack time, grit or knowledge/skill I just couldn&rsquo;t make meaningful progress.</p><p>To be fair - at first it <em>seemed</em> like a simple &ldquo;good first issue&rdquo; kind of thing I had no idea what I was opting into, so here&rsquo;s a disclaimer!
We&rsquo;re going to build a type of database! Specifically an in-memory key-value data store &ndash; like Redis! kinda&mldr; sorta.</p><p>But before that context, here&rsquo;s Joe Armstrong explaining why writing correct, fast, well-tested, concurrent & parallel(distributed) programs on modern CPUs is complex and difficult and why erlang/elixir is appealing, it comes with a concurrent/parallel garbage collector (no global GC pauses, <strong>low-latency by default</strong>), a <strong>shared nothing architecture</strong> that&rsquo;s <strong>multi-core by default</strong> and scales IO bound soft-realtime applications incredibly well with a simple model of concurrency and primitives that encourage thinking about fault tolerance &ndash; did I mention functional programming?</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/bo5WL5IQAd0 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>This covers some wide ranging and complex important topics let&rsquo;s take a peek under the covers of what we say &ldquo;yes&rdquo; to when we want shared
memory concurrency &ndash; spoiler it&rsquo;s hard, but alas we&rsquo;re rebels and rust has <em>fearless concurrency</em> right?:</p><p><img src=/crit.png alt=Danger></p><h2 id=shaping-performance-constraints>Shaping performance constraints</h2><p>It&rsquo;s a worrying premise to write programs in an environment that doesn&rsquo;t have any kind of shared state. In a database for example, you can&rsquo;t just go around copying everything. In the erlang/elixir ecosystem this is solved by leveraging erlang term storage(ets), and it&rsquo;s a key component of the distributed persistent database mnesia all built aware of the language runtime.</p><blockquote><p>It would be very difficult, if not impossible to implement ETS purely in Erlang with similar performance. Due to its reliance on mutable data, the functionality of ETS tables is very expensive to model in a functional programming language like Erlang. [1]</p></blockquote><p>Before we get into the bells and whistles of it all, what is it at its core? Conceputally a key-value store seems simple.
What you want to model is an abstract interface that can store <em>schemaless</em> data and retrieve it fast, essentially a map/dictionary/associative array abstract data type:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>type</span> <span class=nx>Table</span><span class=p>[</span><span class=nx>Key</span> <span class=nx>comparable</span><span class=p>,</span> <span class=nx>Value</span> <span class=nx>any</span><span class=p>]</span> <span class=kd>interface</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nf>Read</span><span class=p>(</span><span class=nx>Key</span><span class=p>)</span> <span class=p>(</span><span class=nx>Value</span><span class=p>,</span> <span class=kt>error</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nf>Write</span><span class=p>(</span><span class=nx>Key</span><span class=p>,</span> <span class=nx>Value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nf>Delete</span><span class=p>(</span><span class=nx>Key</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nf>In</span><span class=p>(</span><span class=nx>Key</span><span class=p>)</span> <span class=kt>bool</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>In terms of an api for this <code>Table</code>, we&rsquo;re looking to define the instance options <code>bag</code>, <code>duplicate_bag</code>, <code>set</code> and <code>ordered_set</code>.
If familiar with <code>table relations</code> in relational databases, think of the key-value mapping like so:</p><ol><li>one to one(1:1) = <code>set</code></li><li>one to one(but with order) = <code>ordered_set</code></li><li>one to many(1:N) = <code>bag</code></li><li>many to many(N:N) <code>duplicate_bag</code></li></ol><p>These in theory allow us to model all sorts of interesting properties like <em>referential integrity</em> - a relationship between two or more tables but we&rsquo;ll get to that <em>later</em>. For now, you might be thinking why not implement this by just throwing a hashmap underneath and that works for types <code>set</code>, <code>bag</code> and <code>duplicate_bag</code>. Infact hashmaps are ubiquitious [4] [5] [6] and contain excellent properties especially when the data set fits in working memory, however most implementations in standard libraries are not thread safe. CPU cores need to synchronize data access to avoid corrupting data or reading inconsistent or stale data.</p><p>In rust - sharing an <code>std::collections::hash_map::HashMap</code> requires wrapping it in two things:</p><ol><li>an atomic reference count <code>Arc&lt;T></code></li><li>a mutex or some other sync mechanism because the type does not impl <code>Send</code> & <code>Sync</code></li></ol><p>If your program and data only has to exist within a single thread of execution that&rsquo;s great, but <em>web servers</em> tend to need to handle <em>concurrent</em> data access. Practical examples of this are caches, rate limiting middleware, session storage, distributed config, simple message queues etc</p><p>Let&rsquo;s wrap it in a mutex from go&rsquo;s std lib&rsquo;s <code>sync</code> package:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>type</span> <span class=nx>Map</span><span class=p>[</span><span class=nx>K</span> <span class=kt>string</span><span class=p>,</span> <span class=nx>V</span> <span class=nx>any</span><span class=p>]</span> <span class=kd>struct</span> <span class=p>{</span>
</span></span><span class=line><span class=cl> <span class=nx>sync</span><span class=p>.</span><span class=nx>Mutex</span>
</span></span><span class=line><span class=cl> <span class=nx>Data</span> <span class=kd>map</span><span class=p>[</span><span class=nx>K</span><span class=p>]</span><span class=nx>V</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>To <code>Read</code> and <code>Write</code> we must acquire <code>*Map.Lock()</code> and release <code>*Map.Unlock()</code>. This works, up to a point &ndash;
but we can do better! We&rsquo;re trying to build a <em>general purpose</em> data store for
key-value data. Global Mutexes are a good solution but you tend to encounter inefficiencies like <em>lock contention</em> on higher values of R/W data access, especially when your hardware can parallelize access when the underlying memory region&rsquo;s slots are partioned perhaps due to hashing across independent memory regions.</p><p>One clever way of getting around this is by using an advanced concurrency technique called fine-grained locking, the general idea is instead of a global mutex we serialise access to specific partitions[1]:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>type</span> <span class=nx>Map</span><span class=p>[</span><span class=nx>K</span> <span class=kt>string</span><span class=p>,</span> <span class=nx>V</span> <span class=nx>any</span><span class=p>]</span> <span class=kd>struct</span> <span class=p>{</span>
</span></span><span class=line><span class=cl> <span class=nx>Data</span>  <span class=kd>map</span><span class=p>[</span><span class=nx>K</span><span class=p>]</span><span class=nx>V</span>
</span></span><span class=line><span class=cl> <span class=nx>locks</span> <span class=p>[]</span><span class=o>*</span><span class=nx>sync</span><span class=p>.</span><span class=nx>Mutex</span>
</span></span><span class=line><span class=cl> <span class=nx>global</span> <span class=nx>sync</span><span class=p>.</span><span class=nx>Mutex</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>This is much more complex but can be more write performant but suffer slightly slower reads. This bottleneck of locks and linearization is the reason databases like postgres and mysql have Multi Version Concurrency Control(MVCC) semantics for pushing reads and writes further using transactions and isolation levels. We&rsquo;ll come back to exploring these fun problems and the tradeoffs and ask the question are locks truely necessary?</p><p>Next, we&rsquo;d like to be able to store both ordered and unordered key value data, hash maps store unordered data so this calls for some sort of additional data structure with fast ordered <code>Table</code> operations for our <code>ordered_set</code>. We must define a new interface:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>type</span> <span class=nx>OrderedTable</span><span class=p>[</span><span class=nx>Key</span> <span class=nx>cmp</span><span class=p>.</span><span class=nx>Ordered</span><span class=p>,</span> <span class=nx>Value</span> <span class=nx>any</span><span class=p>]</span> <span class=kd>interface</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nf>Read</span><span class=p>(</span><span class=nx>Key</span><span class=p>)</span> <span class=p>(</span><span class=nx>Value</span><span class=p>,</span> <span class=kt>error</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nf>Write</span><span class=p>(</span><span class=nx>Key</span><span class=p>,</span> <span class=nx>Value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nf>Delete</span><span class=p>(</span><span class=nx>Key</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nf>In</span><span class=p>(</span><span class=nx>Key</span><span class=p>)</span> <span class=kt>bool</span>
</span></span><span class=line><span class=cl>  <span class=nf>Range</span><span class=p>(</span><span class=nx>Key</span><span class=p>,</span> <span class=nx>Key</span><span class=p>)</span> <span class=p>[]</span><span class=nx>Value</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>For a concrete implementation, let&rsquo;s start with the conceptually simplest/fastest* the Binary Search Tree and a global <code>RWMutex</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>type</span> <span class=nx>BST</span><span class=p>[</span><span class=nx>Key</span> <span class=nx>cmp</span><span class=p>.</span><span class=nx>Ordered</span><span class=p>,</span> <span class=nx>Value</span> <span class=nx>any</span><span class=p>]</span> <span class=kd>struct</span> <span class=p>{</span>
</span></span><span class=line><span class=cl> <span class=nx>key</span>   <span class=nx>Key</span>
</span></span><span class=line><span class=cl> <span class=nx>value</span> <span class=nx>any</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> <span class=nx>parent</span> <span class=o>*</span><span class=nx>BST</span><span class=p>[</span><span class=nx>Key</span><span class=p>,</span> <span class=nx>Value</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=nx>left</span>   <span class=o>*</span><span class=nx>BST</span><span class=p>[</span><span class=nx>Key</span><span class=p>,</span> <span class=nx>Value</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=nx>right</span>  <span class=o>*</span><span class=nx>BST</span><span class=p>[</span><span class=nx>Key</span><span class=p>,</span> <span class=nx>Value</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> <span class=nx>global</span> <span class=nx>sync</span><span class=p>.</span><span class=nx>RWMutex</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Search trees are the &ldquo;go to&rdquo; structure for keeping performant ordered data with balanced read/write performance, by ensuring we keep the &ldquo;search property&rdquo; we can perform on average operations in <code>O(logN)</code> &ndash; if the tree is balanced. Sadly in reality they&rsquo;re bounded by the worst time-complexity of <code>O(h)</code> where h is the height of the tree. What that means is if we get unlucky
with the data - searches can devolve into searching a linked-list. That wouldn&rsquo;t do. Here there are many flavors thankfully.</p><p>Fan favorites include the classics; an AVL Tree, B-Tree or perhaps an LSM Tree, which all come with spices and even more variety.</p><p>In practice we are concerned about much more than order of magnitude choices, we are also interested in how these structures
interact with memory layout, can the data fit in main memory (internal) or is it on disk(external)? is it cache friendly? are the node values blocks of virtual memory or random access? sorting files in a directory is a common example of this problem. What kind of concurrent patterns are enabled? how do they map to our eventual high level API? These questions lead to very different choices in algorithm design.</p><p>What exists in the current erlang runtime system? The data structure chosen previously of which we&rsquo;ll be benchmarking against is something called a <a href=https://www.erlang.org/blog/the-new-scalable-ets-ordered_set/>Contention Adapting Tree</a> [2]. Briefly what&rsquo;s interesting about CA Tree is it dynamically at runtime changes the behaviour and number of locks it holds across the tables it protects depending on nature of contention protecting underneath a sequential ordered data structure such as a treap or AVL Tree.</p><h2 id=concurrency-correctness--going-web-scale>Concurrency, Correctness & Going web scale</h2><p>These days are you a serious software craftsman <a href="https://www.youtube.com/watch?v=b2F-DItXtZs">if you&rsquo;re not at web scale?</a>.</p><p>In our undying, unending pursuit to scale systems further and further we spin webs of complexity. <a href="https://www.youtube.com/watch?v=RlwlV4hcBac">Why? who knows, it&rsquo;s provocative.</a></p><p>Let&rsquo;s put on our scaling cap. Previously we mentioned fine-grained locking as a technique that could lead to better write performance but at the cost of complexity and read performance &ndash; a related application of this technique is called &ldquo;sharding&rdquo;.</p><p>Sharding is a wonderful idea, if one hashmap won&rsquo;t work, let&rsquo;s scale <em>horizontally</em>, have you tried two? or perhaps sixteen or thirty two?
Java&rsquo;s <code>ConcurrentHashMap</code> and rust&rsquo;s <code>DashMap</code> are defacto examples of this. However we need to ask isn&rsquo;t this getting complex? can we still understand the system? most importantly can we guarantee <em>correctness?</em></p><p>As it turns out most databases need to ensure certain guarantees with respect to performance, concurrency and correctness and here we discuss the elusive idea of a &ldquo;transaction&rdquo;. You&rsquo;ve probably heard the acronymn ACID - Atomicity, Consistency, Isolation and Durability. What does that mean for ets? Lucky for us, we can cast away the durability requirement as our data set must fit in working memory(for now). That leaves us with:</p><ul><li>Atomicity</li><li>Consistency</li><li>Isolation</li></ul><h3 id=atomicity>Atomicity</h3><p>At the level of hardware what is atomicity? It&rsquo;s a special instruction set.
An example of an interface to this is go&rsquo;s <a href=https://pkg.go.dev/sync/atomic>sync/atomic</a>.
This instruction gives a certain <em>guarantee</em>,that you can perform an operation without <em>side effects</em> like another process
peeking a half-baked result. You bake a pie or you don&rsquo;t &ndash; however we&rsquo;re getting ahead of ourselves as this part has to do with <em>visibility</em>.</p><p>Now here&rsquo;s where we have to be careful. ETS operations are semantically &ndash; <strong>independently atomic</strong>[9].</p><p>Every operation such as a read, write or multi_write on a table are atomic for a single process. In the relational model
you have <code>BEGIN</code>, <code>COMMIT</code> & <code>ROLLBACK</code> semantics where you can group multiple write operations and pretend they&rsquo;re a single atomic operation. Mnesia builds upon
ets and supports <a href=https://www.erlang.org/doc/apps/mnesia/mnesia_chap4#atomicity>grouping multiple writers</a> with <a href=https://www.erlang.org/doc/man/mnesia#transaction-1>transactions</a> but ets does not.</p><h2 id=isolation>Isolation</h2><p>Isolation is really about how we define the <em>logical concurrent access rules</em> of a <code>Table</code>. In ets we have different access modes for processes:</p><ul><li>public: all processes may read or write.</li><li>protected: all process may read but one exclusive writer.</li><li>private: single reader and writer.</li></ul><p>Why does this matter? Before it was hinted at why the MVCC paradigm [8] exists &ndash; naive locking hurts all query performance, yet locks are desirable
because they ensure correct logical ordering &ndash; linearizability.</p><p>It&rsquo;s worth pausing to consider this for a moment.</p><p>Concurrency is a powerful concept, we can take three logically independent events A, B then C &ndash; potentially reorder them by alternating or <em>interleaving</em>
their execution&rsquo;s progress and reassemble them as A, B then C &ndash; sequential, nice & correct. Systems must be correct, but not necessarily sequential.</p><p>There&rsquo;s a hint of that infamous word here &ndash; a tradeoff, in a concurrent universe performance or correctness pick one? Sadly reality is more complex
and there are different shades on this spectrum that trade one thing by changing the definition of another <a href=https://en.wikipedia.org/wiki/Consistency_model>the devil is in the details</a>.</p><p>What to do? Inline with the <em>more is better</em> philosophy of scaling are (read/write) locking groups, have you tried adding more <em>specialised</em> locks? We can seperate our read access from our writes and scale those patterns somewhat independently &ndash; this is the working principle of <em>snapshot insolation</em>. The concurrency control works by keeping multiple versions on each write and match transactions to specific version of the database at a checkpoint. In a database like postgres you might be familiar with row or table level locks such as <code>FOR UPDATE</code> or <code>ACCESS EXCLUSIVE</code>, in mnesia you have similar <a href=https://www.erlang.org/doc/man/mnesia#lock-2>locking semantics</a>.</p><p>What does this mean for ets? unlike Mnesia ets has no need for MVCC because it does not model the idea of a &ldquo;transaction&rdquo;, nor does it have <a href=https://www.erlang.org/doc/man/mnesia_registry>quorum</a> problems simplifying the implementation and api, nonetheless the ideas of having specialised reader and writer modes provides flexibility to the consumer to make informed choices on what concurrent data patterns make sense in the domain problem being solved.</p><h2 id=consistency>Consistency</h2><p>Consistency is a tricky topic. In a way we can think of <em>referential integrity</em> as a consistent property of a database. You define a primary key and a foreign key and specify a logical relationship between entities based on this &ndash; but really you&rsquo;re defining an interface and specifying a contract with an invariant that must be implemented. ETS does not have referential integrity, check constraints or schema validation, it stores/retrieves data agnostic of the kind or shape and enforces a serializable/linearizable api for concurrent reads and writes to every function API.</p><h2 id=the-dumpster-fire-that-is-garbage-collection>The dumpster fire that is garbage collection</h2><p>So far we&rsquo;ve explored reading and writing data to the <code>Table</code> and <code>OrderedTable</code> but not deletion, what is deletion?</p><p>Deleting data can be thought about as <em>reclaiming</em> and <em>destroying</em>. What happens when a program needs memory? If it&rsquo;s <em>statically known</em> it&rsquo;s usually the compiler&rsquo;s problem. Interfacing with a kernel or raw memory is complex and if a group of smart people can spend alot of time to get it right once and automagically solve it that would be nice indeed. This is the allure of automatic garbage collection. What happens when this model breaks down?</p><p>A brief mention of rust mentioned using atomic reference counts an implementation of <a href=https://doc.rust-lang.org/book/ch15-04-rc.html>reference counting</a> and in go this operation is seemingly automatic and opaque. The resource allocation strategy is tightly coupled to the programming language and environment we intend our concrete key value implementation to eventually live, so at this point we bid farewall to go snippets and explore the problems of lifetimes, alignment & fragementation in rust.</p><h4 id=lifetimes-fragmentation--alignment>Lifetimes, Fragmentation & Alignment</h4><p>What really happens when you dynamically need memory? The compiler throws up its hand decides it <a href=https://en.wikipedia.org/wiki/Undecidable_problem>can&rsquo;t figure it out</a>. You do it.</p><p>When the need arises&mldr; as it often does, you politely ask a kernel for some (and sometimes it says no!), and even when it does say yes, it typically lies to you about what you&rsquo;re getting &ndash; and once you get it, it&rsquo;s this weird stuff that you doesn&rsquo;t make sense to your program and eventually&mldr; you have to give it back otherwise memory keeps growing forever (B)OOM.</p><p>Let&rsquo;s recap:</p><ul><li>You need to keep <em>track of this memory</em> &ndash; it&rsquo;s <em>lifetime</em></li><li>You need to give it back</li></ul><p>As it turns out keeping track of this forms a <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph</a>. It&rsquo;s a huge & complex topic but a brief overview of some common APIs are:</p><ol><li>In C - malloc//free or similar e.g <a href=https://github.com/jemalloc/jemalloc>Jemalloc</a></li><li>reference counting</li><li>the tracing algorithm</li><li>mark and sweep alogrithm</li><li>DIY &lt;&ndash; (we&rsquo;re here, oh no!)</li></ol><p>Why are we resorting to such a low, possibly error prone approach?</p><h4 id=a-detour-for-just-enough-web-assembly>A detour for just enough web assembly</h4><p><a href=https://webassembly.org/>Webassembly</a> is a pretty cool project. The web has four official langauges: html, css, javascript and webassembly. It&rsquo;d be nice
if you could write rust for your browser no? perhaps you&rsquo;d like to ship a runnable binary? Games, figma and containers &ndash; without docker. If this key-value store is going to exists agnostic of wheter it happens to run inside webassembly or <code>x86-64 linux</code> wouldn&rsquo;t that be nice?</p><p>The current ETS exists tightly coupled to the internal of the erlang runtime system (erts) &ndash; ETS has it&rsquo;s has it&rsquo;s private own memory allocator <code>erts_db_alloc</code> and deallocator <code>erts_db_free</code> right on the BEAM virtual machine&rsquo;s in <code>erl_alloc.c</code> via <code>HAlloc</code>. There&rsquo;s far more going on than
we&rsquo;re interested in knowing but the gist is these modules know how to allocate memory on a wide variety of architecture targets and environments.</p><h4 id=stacking-stacks-being-in-the-arena-skipping-and-freeing-lists>Stacking stacks, Being in the Arena, Skipping and Freeing Lists</h4><p>todo:</p><h2 id=more-complex-types>More complex Types</h2><p>(todo: maybe collapse this as a sub header)
So far these examples have been somewhat generic but the underlying implementation only allowed simple types a key of type <code>string</code> and a value of <code>int</code>.
This is intentionally done in order not to distract from other concepts, but if we really want a general key value store we need to allow many types.
In this case we want to allow every type supported by the erlang runtime system known as a <code>Term</code>.</p><h1 id=gotta-go-fast>Gotta Go Fast</h1><p>at what cost?</p><ul><li>use of lock free data structures/behaviour across reads - concurrent skip list crash course, why?</li></ul><p>intro to lock free techniques[3]</p><h2 id=persistence-and-durability>Persistence and Durability</h2><p>ETS has an alternative implementation call Disk-Based Term Storage &ndash; I have no interest in wrastling with the complexities of fsync but for completeness, in theory however would one implement it? To do that we have to re-examine what durability in ACID means.</p><p>Implementing the DETS api</p><blockquote><p>disks have relatively long seek times, reflecting how long it takes the desired part of the disk to rotate under the read/write head. Once the head is in the right place, the data moves relatively quickly, and it costs about the same to read a large data block as it does to read a single byte</p></blockquote><h2 id=the-query-parser>The query parser</h2><p>Every good database needs ergonimics features fo good querying! SQL is amazing but is insanely complex to implement and tightly coupled to transaction semantics,
however we don&rsquo;t want to feel left out, let&rsquo;s build a tiny(compared to sql) query syntax and engine.</p><h2 id=testing--benchmarks>Testing & Benchmarks</h2><ul><li>unit testing challenges, tight coupling etc</li><li>conformance with the upstream erts(erlang runtime system) ETS public api and behaviour</li><li>100% erts TEST SUITE coverage</li></ul><p>methodology, coverage, tools, loom, address sanitizer etc insert graphs of benchmark results</p><h2 id=references>References</h2><ul><li>[1] <a href=http://doi.acm.org/10.1145/2505305.2505308>On the scalability of the Erlang term storage</a></li><li>[2] <a href=https://doi.org/10.1145/2633448.2633455>More Scalable Ordered Set for ETS Using Adaptation</a></li><li>[3] <a href="https://learn.microsoft.com/en-gb/windows/win32/dxtecharts/lockless-programming?redirectedfrom=MSDN">Lockless Programming Considerations for Xbox 360 and Windows</a></li><li>[4] <a href=https://www.postgresql.org/docs/16/hash-intro.html>Hash Indexes in postgres</a></li><li>[5] <a href=https://dev.mysql.com/doc/refman/8.3/en/innodb-adaptive-hash.html>Adaptive Hash Index in mysql</a></li><li>[6] <a href=https://www.postgresql.org/docs/current/hstore.html>Hstore - key/value datatype in postgres</a></li><li>[7] <a href=https://www.postgresql.org/docs/current/index-locking.html>Index Locking in postgres</a></li><li>[8] <a href=https://www.postgresql.org/docs/current/mvcc.html>MVCC introduction</a></li><li>[9] <a href=https://www.erlang.org/doc/man/ets#concurrency>Concurreny in ETS</a></li></ul><h2 id=notes>Notes</h2><blockquote><p>In a reader-writer lock, a read acquisition has to be visible to
writers, so they can wait for the reads to finish before succeeding to take a write lock. One way to implement this is to have
a shared counter that is incremented and decremented atomically
when reading threads are entering and exiting their critical section.</p></blockquote><p><a href=https://preshing.com/20120612/an-introduction-to-lock-free-programming/>https://preshing.com/20120612/an-introduction-to-lock-free-programming/</a></p></article></main><footer id=footer><a href=https://github.com/hailelagi/blog>source</a>
Copyright © 2024 Haile Lagi<div id=sign-key><span>GPG key ID: 0298F4203ADC85E8</span></div></footer></body></html>